\documentclass[a4paper]{article}
\setlength\parindent{0pt}

\usepackage{pgfplots}
\usepackage{amsthm, amsmath, amssymb, verbatim, enumerate, mathtools, algorithm}
\usepackage{pgf}
\usepackage{hyperref}
\def\labelitemi{--}
\pgfplotsset{compat=newest}

\pagestyle{empty}

\title{Deep Q-Network Demo}
\author{Rishikesh Vaishnav}
\begin{document}
\maketitle
\section*{Basic Implementation}
\subsection*{Code}
\begin{itemize}
    \item The code for this project is available at: 
    % TODO
\url{https://github.com/rish987/Reinforcement-Learning/blob/master/demos/deep_q_network/code/deep_q_network.py}.
\end{itemize}
\subsection*{Implementation Details}
\begin{itemize}
    \item Unlike the Atari gameplay environment described by Mnih et. al., the
        pole-cart environment is not perceptually aliased. That is, the current
        observation of the state is theoretically all that is needed to
        determine an optimal value. Therefore, the current state can be equated
        with the current observation, without taking into account past
        observations and actions.
    \item Because the observation space of the Atari gameplay environment is
        much larger than the pole-cart environment, it should suffice to use a
        smaller ANN model.
    \item Because the observation space of the pole-cart environment is small
        and not spatially correlated, it is not helpful to use a convolutional
        neural network.
    \item The model is a simple vanilla neural network with one hidden layer:
    \begin{itemize}
        \item Let $M$ be the number of nodes in the hidden layer.
        \item Let $K$ be the number of output nodes (i.e., number of actions).
        \item The hidden layer is calculated as:
        \begin{align*}
            Z_{m} &= \sigma(\alpha^{T}_{0m} + \alpha^{T}_{m}x(s)), m = 1, \dots, M
        \end{align*}
        \item The output layer is calculated as:
        \begin{align*}
            \hat{q}(s, a_i; \theta) &= \beta_{0i} + \beta^{T}_{i}Z, i = 1, \dots, K
        \end{align*}
    \end{itemize}
    \item Solving for the gradient of the sample error:
        % TODO bias terms?
    \begin{align*}
        \nabla_{\theta}J_t(\theta) &= -2(y_t - \hat{q}(s_t, a_t, \theta))
        \nabla_{\theta}\hat{q}(s_t, a_t, \theta)\\
        \frac{d}{d\beta_{jk}}\hat{q}(s_t, a_t, \theta) &=
        \begin{cases}
            0 & k \ne a_t\\
            \begin{cases}
                1 & j = 0\\
                \sigma(\alpha^{T}_{0j} + \alpha^{T}_{j}x(s_t)) & j > 0
            \end{cases} &  k = a_t
        \end{cases}, k = 1, \dots, K\\
        \frac{d}{d\alpha_{im}}\hat{q}(s_t, a_t, \theta) &=
        \beta_{mk}
        \sigma'(\alpha^{T}_{0m} + \alpha^{T}_{m}x(s_t))
        \begin{cases}
            1 & i = 0\\
            x(s_t)_{i} & i > 0
        \end{cases}
    \end{align*}
\end{itemize}
\subsection*{Results}
\begin{centering}
\end{centering}
\begin{itemize}
    \item The results can be summarized as follows:
    \begin{itemize}
        % TODO
        \item 
    \end{itemize}
\end{itemize}
\end{document}
