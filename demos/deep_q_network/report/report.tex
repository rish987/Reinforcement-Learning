\documentclass[a4paper]{article}
\setlength\parindent{0pt}

\usepackage{pgfplots}
\usepackage{amsthm, amsmath, amssymb, verbatim, enumerate, mathtools, algorithm}
\usepackage{pgf}
\usepackage{hyperref}
\def\labelitemi{--}
\pgfplotsset{compat=newest}

\pagestyle{empty}

\title{Deep Q-Network Demo}
\author{Rishikesh Vaishnav}
\begin{document}
\maketitle
\section*{Basic Implementation}
\subsection*{Code}
Manual Implementation
\begin{itemize}
    \item The code for this project is available at: 
    % TODO
    \url{https://github.com/rish987/Reinforcement-Learning/blob/master/demos/deep_q_network/code/deep_q_network.py}.
\end{itemize}
Keras Implementation
\begin{itemize}
    \item The code for this project is available at: 
    % TODO
    \url{https://github.com/rish987/Reinforcement-Learning/blob/master/demos/deep_q_network/code/deep_q_network_keras.py}.
\end{itemize}
\subsection*{Implementation Details}
\begin{itemize}
    \item Unlike the Atari gameplay environment described by Mnih et. al., the
        pole-cart environment is not perceptually aliased. That is, the current
        observation of the state is theoretically all that is needed to
        determine an optimal value. Therefore, the current state can be equated
        with the current observation, without taking into account past
        observations and actions.
    \item Because the observation space of the Atari gameplay environment is
        much larger than the pole-cart environment, it should suffice to use a
        smaller ANN model.
    \item Because the observation space of the pole-cart environment is small
        and not spatially correlated, it is not helpful to use a convolutional
        neural network.
\end{itemize}
Manual Implementation
\begin{itemize}
    \item The model is a simple vanilla neural network with one hidden layer:
    \begin{itemize}
        \item Let $M$ be the number of nodes in the hidden layer.
        \item Let $K$ be the number of output nodes (i.e., number of actions).
        \item Let $\sigma(x)$ be the sigmoid activation function
            $\frac{1}{1 + e^{-x}}$.
        \item The hidden layer is calculated as:
        \begin{align*}
            Z_{m} &= \sigma(\alpha^{T}_{0m} + \alpha^{T}_{m}x(s)), m = 1, \dots, M
        \end{align*}
        \item The output layer is calculated as:
        \begin{align*}
            \hat{q}(s, a_i; \theta) &= \beta_{0i} + \beta^{T}_{i}Z, i = 1, \dots, K
        \end{align*}
    \end{itemize}
    \item Solving for the gradient of the sample error:
    \begin{align*}
        \nabla_{\theta}J_t(\theta) &= -2(y_t - \hat{q}(s_t, a_t, \theta))
        \nabla_{\theta}\hat{q}(s_t, a_t, \theta)\\
        \frac{d}{d\beta_{jk}}\hat{q}(s_t, a_t, \theta) &=
        \begin{cases}
            0 & k \ne a_t\\
            \begin{cases}
                1 & j = 0\\
                \sigma(\alpha^{T}_{0j} + \alpha^{T}_{j}x(s_t)) & j > 0
            \end{cases} &  k = a_t
        \end{cases}, k = 1, \dots, K\\
        \frac{d}{d\alpha_{im}}\hat{q}(s_t, a_t, \theta) &=
        \beta_{mk}
        \sigma'(\alpha^{T}_{0m} + \alpha^{T}_{m}x(s_t))
        \begin{cases}
            1 & i = 0\\
            x(s_t)_{i} & i > 0
        \end{cases}
    \end{align*}
    \item A decaying $\epsilon$ was used, which started at $1.0$ and decayed to
        a minimum value of $0.01$.
    \item A constant decay rate was used for the learning rate $\alpha$.
Keras Implementation
\begin{itemize}
    \item The size of the model seemed to significantly affect the performance
        of the Keras ANN. I found that a model that yielded good (though
        perhaps not optimal) results contained two hidden layers of 24 nodes
        each, with ReLU activation functions.
    \item Other than replacing the model and outsourcing gradient calculation,
        the general algorithmic framework remained the same.
\end{itemize}
\end{itemize}
\subsection*{Results}
Manual Implementation

\begin{centering}
    \scalebox{0.6}{\input{DQN_manual.pgf}} \\
\end{centering}
\begin{itemize}
    \item The results for different $\alpha$ can be summarized as follows:
    \begin{itemize}
        \item The largest $\alpha$ learned initially, but worsened and
            failed to find a policy that was capable of passing the episode at
            all. This suggests that, in starting with a larger $\alpha$, it
            could be useful to decay by a larger factor.
        \item The middle $\alpha$ learned very well initially, but 
            worsened and failed to find a policy that was capable of
            consistently passing the episode.  This also suggests that, in
            starting with a larger $\alpha$, it could be useful to decay by a
            larger factor.
        \item The small $\alpha$ learned slowly, but eventually leveled off at
            a better policy than those of the other $\alpha$s. This may mean
            that the decay rate was too large relative to this $\alpha$, causing
            it to cease improving after $\alpha$ became negligibly small.
    \end{itemize}
    \item These results suggest that this learner is feasable, and in order to
        improve this learner, it is necessary to more carefully control how
        $\alpha$ decays over time. It may also be useful to increase the number
        of hidden layers in the model. 
    \item To verify that this was the case,
        I move on to replacing my manual implementation with a Keras 
        implementation that handles training on its own and with which I can
        easily change the ANN parameters.
\end{itemize}
Keras Implementation

\begin{centering}
    \scalebox{0.6}{\input{DQN_Keras.pgf}} \\
\end{centering}
Note: Because Keras took significantly longer to run, I didn't collect enough
data to average, which explains the noise. However, I observed that the
smallest learning rate ($\alpha = 0.001$) consistently scored $> 150$ on
subsequent runs.
\begin{itemize}
    \item The results for different $\alpha$ can be summarized as follows:
    \begin{itemize}
        \item The largest $\alpha$ performed poorly initially, but learned a
            policy that generally performed worse than the other two $\alpha$s.
        \item The middle $\alpha$ Learned quickly, and performed averagely
            compared to the other two $\alpha$s.
        \item The smallest $\alpha$ learned quickly, and performed relatively
            well, converging to an almost optimal policy.
    \end{itemize}
    \item This clearly outperforms my manual implementation, but when I
        reduced the model to approximately the size of my manually constructed
        model, it performed similarly. This suggests that the size of the
        neural network and intelligent control of the learning rate can
        significantly affect the performance of the algorithm.
\end{itemize}
\end{document}
