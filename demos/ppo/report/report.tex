\documentclass[a4paper]{article}
\setlength\parindent{0pt}

\usepackage{pgfplots}
\usepackage{amsthm, amsmath, amssymb, verbatim, enumerate, mathtools, algorithm}
\usepackage{pgf}
\usepackage{hyperref}
\def\labelitemi{--}
\def\labelitemii{--}
\def\labelitemiii{--}
\def\labelitemiv{--}
\pgfplotsset{compat=newest}

\pagestyle{empty}

\title{Proximal Policy Optimization Demo}
\author{Rishikesh Vaishnav}
\begin{document}
\maketitle
\subsection*{Code}
\begin{itemize}
    \item The code for this project is available at: 
    \url{https://github.com/rish987/Reinforcement-Learning/blob/master/demos/ppo/code/ppo.py}.
\end{itemize}
\subsection*{Implementation Details}
Pseudocode:
\begin{itemize}
    \item Initialize policy parameter $\theta$.
    \item Iterate until convergence:
    \begin{itemize}
        \item Initialize/clear list $S$ of $\{G, \pi_{\theta}(s, a), s, a\}$.
        \item Generate $N_{\tau}$ trajectories $\{\tau\}$, saving 
            $\pi_{\theta}(s, a)$ for each $s, a$ encountered.
        \item For each trajectory $\tau \in \{\tau\}$:
        \begin{itemize}
            \item For each $\{s, a\} \in \tau$:
            \begin{itemize}
                \item Calculate discounted return $G_{\theta}(s, a)$ from this
                    time to end of episode.
                \item Retrieve $\pi_{\theta}(a | s)$ at this $(s, a)$.
                \item Store $(G, \pi_{\theta}(a | s), s, a)$ in $S$.
            \end{itemize}
        \end{itemize}
        \item Use automatic differentiation library to calculate the gradient
            $\nabla_{\theta}L^{CLIP}(\theta)$ of:
        \begin{align*}
            L^{CLIP}(\theta) &= \frac{1}{|S|}\sum_{(G, \pi_{\theta_{old}}(a |
            s), s, a) \in S} min\left(\frac{\pi_{\theta}(a |
            s)}{\pi_{\theta_{old}}(a | s)}G, clip\left(\frac{\pi_{\theta}(a |
            s)}{\pi_{\theta_{old}}(a | s)}, 1 - \epsilon, 1 +
            \epsilon\right)G\right)
        \end{align*}
        \item $\theta = \theta + \alpha\nabla_{\theta}L^{CLIP}(\theta)$.
    \end{itemize}
\end{itemize}
Parameter Settings:
\begin{itemize}
    \item $\epsilon = 0.2$ (as per Schulman et. al.)
    \item $\gamma = 1$ (adjusted to maximize empirical performance)
\end{itemize}
Policy Function Encoding:
\begin{itemize}
    \item Each state-action pair is converted to the feature vector $x(s, a)$.
        Letting $S_{obs}$ and $S_{act}$ be the size of the observation and
        action spaces, respectively, the size of the vector is $S_{obs} \times
        S_{act}$, where all features are $0$ except for the $S_{obs}$ features
        starting at index $S_{obs} \times a$, which are set to the
        environment's parameterization of $s$.
    \begin{itemize}
        \item In this case, $S_{obs} = 4$ and $S_{act} = 2$.
    \end{itemize}
    \item The policy function $\pi(a | s, \theta)$ performs the softmax on a
        parameterized linear mapping of feature vectors:
        \begin{align*}
            \pi(a | s, \theta) &= \frac{e^{\theta^T x(s, a)}}
            {\sum_{b} e^{\theta^T x(s, b)}}\\
        \end{align*}
    \item Gradient calculation was performed on the objective using the 
        autograd library. Various constant learning rates were tested.
\end{itemize}
\subsection*{Results}
\begin{centering}
    \scalebox{0.6}{\input{PPO_agent.pgf}} \\
\end{centering}
\begin{itemize}
    \item Clearly, the learning rate had a significant effect on performance.
        Smaller learning rates were prohibitive, but after the learning rate
        passed a certain threshold, the problem became feasable.
    \item It is important to note that the sample complexity is far inferior to
        TRPO. This may reflect a poor choice in hyperparameters more than an
        inherent weakness in the algorithm. However, because the update at each
        iteration was very simple, the code ran significantly faster.
    \item Possible areas of improvement include a dynamic learning rate, more
        effective trajectory per iteration count, and a different epsilon.
\end{itemize}
\end{document}
