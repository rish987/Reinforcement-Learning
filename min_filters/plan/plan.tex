\documentclass[a4paper]{article}
\setlength\parindent{0pt}

\usepackage{pgfplots}
\usepackage{amsthm, amsmath, amssymb, verbatim, enumerate, mathtools, algorithm}
\usepackage{pgf}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{hyperref}
\def\labelitemi{--}
\pgfplotsset{compat=newest}
\newcommand{\eps}{0.2}
\newcommand{\gp}{1}
\newcommand{\gn}{-1}
\newcommand{\gaax}[3]{
    \begin{axis}[ 
		title=#2,
        title style = {font=\scriptsize},
        xlabel=$r_t(\theta)$,
        ymin=#1 - 1, ymax=#1 + 1,
        xmin=0, xmax=2,
        width=7cm,height=5cm,
        legend pos = #3,
        legend style = {font=\tiny},
        xtick = {1, 1 + \eps, 1 - \eps},
        xtick pos = bottom,
        xticklabels = {$1$, $1 + \epsilon$, $1 - \epsilon$},
        ytick pos = bottom,
        ytick = {#1},
        yticklabels = {$\hat{G}_t$}
    ] 
}
\newcommand{\minproc}[2]{
    \begin{tikzpicture}
        [
        declare function={
            mfg(\x) = 
                ((\x<=1 - \eps) * (1 - \eps) +
                and(\x>1 - \eps, \x<=1 + \eps) * (\x) +
                (\x>1 + \eps) * (1 + \eps)) * #1;
            rg(\x) = \x * #1;
        }
        ]
        \gaax{#1}{
            {$r_t\hat{G}_t$ and $m(r_t)\hat{G}_t$}
        }{#2}
        \addplot [color=red, mark=none, samples=200] {mfg(x)}; 
        \addplot [color=blue, mark=none, samples=200] {rg(x)}; 
        \draw [dashed] (0,#1) -| (1,#1 - 1);
        \legend{$r_t\hat{A}_t$, {$m(r_t)\hat{A}_t$}}
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
        [
        declare function={
            c(\x) = 
                min(((\x<=1 - \eps) * (1 - \eps) +
                and(\x>1 - \eps, \x<=1 + \eps) * (\x) +
                (\x>1 + \eps) * (1 + \eps)) * #1
                ,\x * #1);
        }
        ]
        \gaax{#1}{
            {$\min(r_t\hat{G}_t, m(r_t)\hat{G}_t)$}
        }{#2}
        \addplot [color=black, mark=none, samples=200] {c(x)}; 
        \draw [dashed] (0,#1) -| (1,#1 - 1);
        \end{axis}
    \end{tikzpicture}
}

\pagestyle{empty}

\title{Research Plan}
\author{\vspace{-5ex}}
\date{\vspace{-5ex}}
\begin{document}
\captionsetup{labelformat=empty}
\maketitle
\section*{Area of Focus}
\begin{itemize}
    \item In their paper on Proximal Policy Optimization, Schulman et. al.
        \cite{ppo}
        propose the clipped surrogate loss function for a fixed 
        parameter $\epsilon$:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t(\theta)\hat{A}_t, \text{clip}
        (r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t\right) \right]
    \end{align*}
	where 
	$r_t(\theta) = 
	\frac
	{\pi_{\theta}(a_t | s_t)}
	{\pi_{\theta_{old}} (a_t | s_t)}$
    and $\hat{A}_t$ is the generalized
    advantage estimator. For simplicity, let $r_t(\theta) = r_t$. 
    $\hat{A}_t$ can be replaced with a number of other
    ``$\gamma$-just'' estimators that must satisfy certain conditions
    \cite{gae}
	.  
    Generalizing $\hat{A}_t$ to these estimators, which will be
    denoted $\hat{G}_t$, yields:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t\hat{G}_t, \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)\hat{G}_t\right) \right]
    \end{align*}
    \item The goal is to investigate replacements for the clipper
        function \\ $\text{clip} (r_t, 1 - \epsilon, 1 + \epsilon)$.
        Let us refer to these replacements as ``min-filters,'' and let $m(r_t)$
        denote an arbitrary min-filter.
    \item In this experimental framework, we have the loss function:
    \begin{align*}
        L^{m}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t(\theta)\hat{G}_t, m(r_t)\hat{G}_t\right) \right]
    \end{align*}
    \item $L^{CLIP}$ is simply an instance of this where $m(r_t) = \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)$.

    \newpage
    \item
        Illustrating minimization under $L_{CLIP}$ on individual expectation
        components:
    \begin{figure}[H]
        % using advantages ensures better mix of positive and negative
        \minproc{\gp}{north west}
        \caption{Expectation component, $\hat{G}_t > 0$}
    \end{figure}
    \begin{figure}[H]
        \minproc{\gn}{north east}
        \caption{Expectation component, $\hat{G}_t < 0$}
    \end{figure}
    \item The paper on Trust Region Policy Optimization by Schulman et. al.
        \cite{trpo}
        proposes a target function whose maximization guarantees monotonic
        improvement:
        \begin{align*}
            targ(\theta) &= L_{\theta_{old}}(\theta) 
            - CD_{KL}^{max}(\theta, \theta_{old})
        \end{align*}
        where $C$ is a fixed positive constant (see paper for specifics) and it
        is shown that 
        \begin{align*}
            L_{\theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{s \sim p_{\theta_{old}}, a \sim \theta_{old}}
            \left[
            \frac
            {\pi_{\theta}(a | s)}
            {\pi_{\theta_{old}} (a | s)}
            A_{\theta_{old}}(s, a)
            \right]
        \end{align*}
        where $p_{\theta_{old}}$ is the normalized discounted visitation
        frequency distribution.
        \item Assuming that the on-policy distribution matches the normalized
            discounted visitation frequency distribution, we can write:
        % TODO double-check above assumption
        \begin{align*}
            L_{\theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{t \in (1, \dots \infty)}
            \left[ r_{t} \hat{A}_t \right]
        \end{align*}
    \item By definition, any $\gamma$-just estimator can replace $\hat{A}_t$
        because doing so only adds a constant to $targ(\theta)$.
        %(TODO: double-check that $\gamma$-just theory still applies in this 
        %situation). 
        Therefore, we can redefine $L_{\theta_{old}}(\theta)$ as:
        \begin{align*}
            L_{g, \theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{t \in (1, \dots \infty)}
            \left[ r_{t} \hat{G}_t \right]
        \end{align*}
    \item Plugging into the target function, multiplying by $1 - \gamma$, and
        absorbing $1 - \gamma$ into $C$ leaves us with the
        gradient-equivalent target function:
    \begin{align*}
        targ_{g}(\theta) &= 
            \mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - CD_{KL}^{max}(\theta, \theta_{old})\\
            \nabla_{\theta} targ_{g}(\theta) &= \nabla_{\theta} targ(\theta) 
    \end{align*}
    \item Consider the case where $\forall t \in (1, \dots \infty)$, $\hat{G}_t
        > 0$ and $r_t < 1 + \epsilon$ and let $\theta \ne \theta_{old}$. In
        this case, no penalty is applied and the clipped loss is a strict
        overestimate without the same gradient:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t\hat{G}_t, \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)\hat{G}_t\right) \right]\\
        &= \hat{\mathbb{E}}_{t}\left[ 
        r_t\hat{G}_t\right]\\
        &\ge \mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - CD_{KL}^{max}(\theta, \theta_{old})\\
        &= targ_{g}(\theta)\\
        \nabla_{\theta}L^{CLIP}(\theta) &= \nabla_{\theta}
        \hat{\mathbb{E}}_{t}\left[r_t\hat{G}_t\right]\\
        &\ne \nabla_{\theta}\mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - C\nabla_{\theta}D_{KL}^{max}(\theta, \theta_{old})\\
        &= \nabla_{\theta}targ_{g}(\theta)
    \end{align*}
    \item Removing the assumption that $\hat{G}_t > 0$, the above still holds
        only if, for all positive $\hat{G}_t$, $r_t < 1 + \epsilon$, and for
        all negative $\hat{G}_t$, $r_t > 1 - \epsilon$. 
    \item If $r_t$ is independent of the sign of $\hat{G}_t$, this is generally
        a harder condition to meet. Experimentally, I found that, on almost
        every batch, the number of timesteps $t$ where
        $r_t < 1 + \epsilon$ was greater than the number of timesteps where
        ($\hat{A}_t < 0$ and $r_t > 1 - \epsilon$) or ($\hat{A}_t > 0$ and 
        $r_t < 1 + \epsilon$). This means that, if $\hat{G}_t$ can be
        both positive and negative, penalties become more possible, allowing
        $L^{CLIP}(\theta)$ to better approximate $targ_{g}(\theta)$, better
        guaranteeing monotonic improvement.
    \item This reasoning could explain the preference for
        advantage estimators over value estimators, because the condition that 
        $\mathbb{E}_t(\hat{A}_t) = 0$ requires that advantage estimators be
        negative half the time, while value functions are typically always
        positive or always negative.
    \item Research question: In some cases, it is simpler to implement a value
        estimator than an advantage estimator. Can we design a min-filter that
        specifically addresses the above concerns to make it more feasable to
        use a value estimator in Proximal Policy Optimization?
        
        %Therefore, if it tends
        %to be the case that $r_t < 1$ independent of the sign of $\hat{G}_t$,
        %and $\mathbb{E}_t(\hat{G}_t) = 0$, the condition is less likely to be
        %met than if we required $\hat{G}_t > 0$.
        % TODO might not need. This is guaranteed if,
        %for all $t$, $1 - \epsilon < r_t < 1 + \epsilon$.
\end{itemize}
\newpage
\section*{$L^{CLIP}$ Penalty Differences for Different Estimate Models}
\begin{itemize}
    % TODO put hats on \hat{G}s
    \item Consider the set of expectation-component parameters $((r_1, \hat{G}_1),
        (r_2, \hat{G}_2), \dots (r_T, \hat{G}_T))$, where all $r$ and $\hat{G}$
        are uncorrelated.
    \item Under $L^{CLIP}$, a particular timestep $t$ will be penalized in
        either of two cases:
    \begin{itemize}
        \item $\hat{G}_t$ is positive and $r_t > 1 + \epsilon$.
        \item $\hat{G}_t$ is negative and $r_t < 1 - \epsilon$.
    \end{itemize}
    \item Therefore, by the assumption of independence of $\hat{G}_t$ and
        $r_t$, we have the expected number of penalized timesteps:\\
        $(p(\hat{G}_t > 0)p(r_t > 1 + \epsilon) + p(\hat{G}_t < 0)p(r_t < 1 -
        \epsilon))T$.
    \item Let $\hat{G}_{1, t}$ and $\hat{G}_{2, t}$ be two alterate estimators.
        To understand differences in the expected number of penalized timesteps
        as we modify the sign of $\hat{G}$, define the ratio:
    \begin{align*}
        r_{diff} &= 
        \frac
        {(p(\hat{G}_{1, t} > 0)p(r_t > 1 + \epsilon) + p(\hat{G}_{1, t} <
        0)p(r_t < 1 - \epsilon))T}
        {(p(\hat{G}_{2, t} > 0)p(r_t > 1 + \epsilon) + p(\hat{G}_{2, t} <
        0)p(r_t < 1 - \epsilon))T}\\
        &=
        \frac
        {p(\hat{G}_{1, t} > 0)p(r_t > 1 + \epsilon) + p(\hat{G}_{1, t} <
        0)p(r_t < 1 - \epsilon)}
        {p(\hat{G}_{2, t} > 0)p(r_t > 1 + \epsilon) + p(\hat{G}_{2, t} <
        0)p(r_t < 1 - \epsilon)}
    \end{align*}
    \item If $p(r_t > 1 + \epsilon) = p(r_t < 1 - \epsilon)$, this ratio
        degenerates to $1$ regardless of the sign distributions of $\hat{G}_1$
        and $\hat{G}_2$.
    \item Consider the example where 
        $p(\hat{G}_{1, t} > 0) = p(\hat{G}_{1, t} < 0) = 0.5$
        and
        $p(\hat{G}_{2, t} > 0) = 1$, $p(\hat{G}_{2, t} < 0) = 0$. Finding the
        conditions under which $r_{diff} > 1$:
        \begin{align*}
            r_{diff} &>  1\\
            \frac{0.5(p(r_t > 1 + \epsilon) + p(r_t < 1 - \epsilon))}
                {p(r_t > 1 + \epsilon)}
            &> 1\\
            \frac{p(r_t > 1 + \epsilon) + p(r_t < 1 - \epsilon)}
                {p(r_t > 1 + \epsilon)}
            &> 2\\
            p(r_t > 1 + \epsilon) + p(r_t < 1 - \epsilon)
            &> 2p(r_t > 1 + \epsilon)\\
            p(r_t < 1 - \epsilon)
            &> p(r_t > 1 + \epsilon)
        \end{align*}
    \item Consider a continuous action space and gaussian
        policies with trainable but state-independent standard deviations.
    \item In general, on a particular state $s$, the standard deviation encoded
        by $\theta$ will decrease as the agent becomes more certain of its
        actions, and the mean will get further from the mean encoded by
        $\theta_{old}$. Visualizing the overlaid gaussians, both of these
        actions will make it more likely that the above condition is true.
    \item Therefore, as training progresses in a single iteration, we expect
        that the $\hat{G}_1$ estimate will begin to induce penalties on more
        timesteps than the $\hat{G}_2$ estimate.
    \item Following similar logic as above, we have that if
        $p(\hat{G}_{2, t} > 0) = 0$, $p(\hat{G}_{2, t} < 0) = 1$, the condition
        $r_{diff} > 1$ requires that $p(r_t < 1 - \epsilon) < p(r_t > 1 +
        \epsilon)$.  However, because we have just reasoned that the opposite
        relation tends to be true as an iteration progresses, it must be be the
        case that $r_{diff} < 1$ - that is, using such an estimator results in
        more penalized timesteps.
    \item Testing this theory empirically on the InvertedPendulum-v2
        environment with a standard PPO agent and an advatage estimate
        $\hat{G}_t$, I observed that, in a single
        iteration, the number of $(\hat{G}_t, r_t)$ where 
        ($\hat{G}_t > 0 $ and $r_t > 1 + \epsilon$)
        or
        ($\hat{G}_t < 0 $ and $r_t < 1 - \epsilon$) became consitently greater
        than the number of $r_t$ where $r_t > 1 + \epsilon$, and consistently
        less than the number of $r_t$ where $r_t < 1 - \epsilon$. This is in
        agreement with the above theoretical results.
\end{itemize}

%\newpage
%\section*{Ideas and Intuitions}
%\subsection*{Sigmoid Min-Filters}
%
%\newpage
%\section*{Plan}

\newpage
\begin{thebibliography}{9}
\bibitem{ppo} 
\url{https://arxiv.org/abs/1707.06347}
\bibitem{gae} 
\url{https://arxiv.org/abs/1506.02438}
\bibitem{trpo} 
\url{https://arxiv.org/abs/1502.05477}
\end{thebibliography}

\end{document}
