\documentclass[a4paper]{article}
\setlength\parindent{0pt}

\usepackage{pgfplots}
\usepackage{amsthm, amsmath, amssymb, verbatim, enumerate, mathtools, algorithm}
\usepackage{pgf}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{hyperref}
\def\labelitemi{--}
\pgfplotsset{compat=newest}
\newcommand{\eps}{0.2}
\newcommand{\gp}{1}
\newcommand{\gn}{-1}
\newcommand{\gaax}[3]{
    \begin{axis}[ 
		title=#2,
        title style = {font=\scriptsize},
        xlabel=$r_t(\theta)$,
        ymin=#1 - 1, ymax=#1 + 1,
        xmin=0, xmax=2,
        width=7cm,height=5cm,
        legend pos = #3,
        legend style = {font=\tiny},
        xtick = {1, 1 + \eps, 1 - \eps},
        xtick pos = bottom,
        xticklabels = {$1$, $1 + \epsilon$, $1 - \epsilon$},
        ytick pos = bottom,
        ytick = {#1},
        yticklabels = {$\hat{G}_t$}
    ] 
}
\newcommand{\minproc}[2]{
    \begin{tikzpicture}
        [
        declare function={
            mfg(\x) = 
                ((\x<=1 - \eps) * (1 - \eps) +
                and(\x>1 - \eps, \x<=1 + \eps) * (\x) +
                (\x>1 + \eps) * (1 + \eps)) * #1;
            rg(\x) = \x * #1;
        }
        ]
        \gaax{#1}{
            {$r_t\hat{G}_t$ and $m(r_t)\hat{G}_t$}
        }{#2}
        \addplot [color=red, mark=none, samples=200] {mfg(x)}; 
        \addplot [color=blue, mark=none, samples=200] {rg(x)}; 
        \draw [dashed] (0,#1) -| (1,#1 - 1);
        \legend{$r_t\hat{A}_t$, {$m(r_t)\hat{A}_t$}}
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
        [
        declare function={
            c(\x) = 
                min(((\x<=1 - \eps) * (1 - \eps) +
                and(\x>1 - \eps, \x<=1 + \eps) * (\x) +
                (\x>1 + \eps) * (1 + \eps)) * #1
                ,\x * #1);
        }
        ]
        \gaax{#1}{
            {$\min(r_t\hat{G}_t, m(r_t)\hat{G}_t)$}
        }{#2}
        \addplot [color=black, mark=none, samples=200] {c(x)}; 
        \draw [dashed] (0,#1) -| (1,#1 - 1);
        \end{axis}
    \end{tikzpicture}
}

\pagestyle{empty}

\title{Research Plan}
\author{\vspace{-5ex}}
\date{\vspace{-5ex}}
\begin{document}
\captionsetup{labelformat=empty}
\maketitle
\section*{Area of Focus}
\begin{itemize}
    \item In their paper on Proximal Policy Optimization, Schulman et. al.
        \cite{ppo}
        propose the clipped surrogate loss function for a fixed 
        parameter $\epsilon$:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t(\theta)\hat{A}_t, \text{clip}
        (r_t(\theta), 1 - \epsilon, 1 + \epsilon)\hat{A}_t\right) \right]
    \end{align*}
	where 
	$r_t(\theta) = 
	\frac
	{\pi_{\theta}(a_t | s_t)}
	{\pi_{\theta_{old}} (a_t | s_t)}$
    and $\hat{A}_t$ is the generalized
    advantage estimator. For simplicity, let $r_t(\theta) = r_t$. 
    $\hat{A}_t$ can be replaced with a number of other
    ``$\gamma$-just'' estimators that must satisfy certain conditions
    \cite{gae}
	.  
    Generalizing $\hat{A}_t$ to these estimators, which will be
    denoted $\hat{G}_t$, yields:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t\hat{G}_t, \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)\hat{G}_t\right) \right]
    \end{align*}
    \item The goal is to investigate replacements for the clipper
        function \\ $\text{clip} (r_t, 1 - \epsilon, 1 + \epsilon)$.
        Let us refer to these replacements as ``min-filters,'' and let $m(r_t)$
        denote an arbitrary min-filter.
    \item In this experimental framework, we have the loss function:
    \begin{align*}
        L^{m}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t(\theta)\hat{G}_t, m(r_t)\hat{G}_t\right) \right]
    \end{align*}
    \item $L^{CLIP}$ is simply an instance of this where $m(r_t) = \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)$.

    \newpage
    \item
        Illustrating minimization under $L_{CLIP}$ on individual expectation
        components:
    \begin{figure}[H]
        % using advantages ensures better mix of positive and negative
        \minproc{\gp}{north west}
        \caption{Expectation component, $\hat{G}_t > 0$}
    \end{figure}
    \begin{figure}[H]
        \minproc{\gn}{north east}
        \caption{Expectation component, $\hat{G}_t < 0$}
    \end{figure}
    \item The paper on Trust Region Policy Optimization by Schulman et. al.
        \cite{trpo}
        proposes a target function whose maximization guarantees monotonic
        improvement:
        \begin{align*}
            targ(\theta) &= L_{\theta_{old}}(\theta) 
            - CD_{KL}^{max}(\theta, \theta_{old})
        \end{align*}
        where $C$ is a fixed positive constant (see paper for specifics) and it
        is shown that 
        \begin{align*}
            L_{\theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{s \sim p_{\theta_{old}}, a \sim \theta_{old}}
            \left[
            \frac
            {\pi_{\theta}(a | s)}
            {\pi_{\theta_{old}} (a | s)}
            A_{\theta_{old}}(s, a)
            \right]
        \end{align*}
        where $p_{\theta_{old}}$ is the normalized discounted visitation
        frequency distribution.
        \item Assuming that the on-policy distribution matches the normalized
            discounted visitation frequency distribution, we can write:
        % TODO double-check above assumption
        \begin{align*}
            L_{\theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{t \in (1, \dots \infty)}
            \left[ r_{t} \hat{A}_t \right]
        \end{align*}
    \item By definition, any $\gamma$-just estimator can replace $\hat{A}_t$
        because doing so only adds a constant to $targ(\theta)$.
        %(TODO: double-check that $\gamma$-just theory still applies in this 
        %situation). 
        Therefore, we can redefine $L_{\theta_{old}}(\theta)$ as:
        \begin{align*}
            L_{g, \theta_{old}}(\theta) &= \frac{1}{1 - \gamma}
            \mathbb{E}_{t \in (1, \dots \infty)}
            \left[ r_{t} \hat{G}_t \right]
        \end{align*}
    \item Plugging into the target function, multiplying by $1 - \gamma$, and
        absorbing $1 - \gamma$ into $C$ leaves us with the
        gradient-equivalent target function:
    \begin{align*}
        targ_{g}(\theta) &= 
            \mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - CD_{KL}^{max}(\theta, \theta_{old})\\
            \nabla_{\theta} targ_{g}(\theta) &= \nabla_{\theta} targ(\theta) 
    \end{align*}
    \item Consider the case where $\forall t \in (1, \dots \infty)$, $\hat{G}_t
        > 0$ and $r_t < 1 + \epsilon$ and let $\theta \ne \theta_{old}$. In
        this case, no penalty is applied and the clipped loss is a strict
        overestimate without the same gradient:
    \begin{align*}
        L^{CLIP}(\theta) &= \hat{\mathbb{E}}_{t}\left[ 
        \min\left(r_t\hat{G}_t, \text{clip}
        (r_t, 1 - \epsilon, 1 + \epsilon)\hat{G}_t\right) \right]\\
        &= \hat{\mathbb{E}}_{t}\left[ 
        r_t\hat{G}_t\right]\\
        &\ge \mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - CD_{KL}^{max}(\theta, \theta_{old})\\
        &= targ_{g}(\theta)\\
        \nabla_{\theta}L^{CLIP}(\theta) &= \nabla_{\theta}
        \hat{\mathbb{E}}_{t}\left[r_t\hat{G}_t\right]\\
        &\ne \nabla_{\theta}\mathbb{E}_{t}
            \left[ r_{t} \hat{G}_t \right] 
            - C\nabla_{\theta}D_{KL}^{max}(\theta, \theta_{old})\\
        &= \nabla_{\theta}targ_{g}(\theta)
    \end{align*}
    \item Removing the assumption that $\hat{G}_t > 0$, the above still holds
        only if, for all positive $\hat{G}_t$, $r_t < 1 + \epsilon$, and for
        all negative $\hat{G}_t$, $r_t > 1 - \epsilon$. 
    \item If $r_t$ is independent of the sign of $\hat{G}_t$, this is generally
        a harder condition to meet. Experimentally, I found that, on almost
        every batch, the number of timesteps $t$ where
        $r_t < 1 + \epsilon$ was greater than the number of timesteps where
        ($\hat{A}_t < 0$ and $r_t > 1 - \epsilon$) or ($\hat{A}_t > 0$ and 
        $r_t < 1 + \epsilon$). This means that, if $\hat{G}_t$ can be
        both positive and negative, penalties become more possible, allowing
        $L^{CLIP}(\theta)$ to better approximate $targ_{g}(\theta)$, better
        guaranteeing monotonic improvement.
    \item This reasoning could explain the preference for
        advantage estimators over value estimators, because the condition that 
        $\mathbb{E}_t(\hat{A}_t) = 0$ requires that advantage estimators be
        negative half the time, while value functions are typically always
        positive or always negative.
    \item Research question: In some cases, it is simpler to implement a value
        estimator than an advantage estimator. Can we design a min-filter that
        specifically addresses the above concerns to make it more feasable to
        use a value estimator in Proximal Policy Optimization?
        
        %Therefore, if it tends
        %to be the case that $r_t < 1$ independent of the sign of $\hat{G}_t$,
        %and $\mathbb{E}_t(\hat{G}_t) = 0$, the condition is less likely to be
        %met than if we required $\hat{G}_t > 0$.
        % TODO might not need. This is guaranteed if,
        %for all $t$, $1 - \epsilon < r_t < 1 + \epsilon$.
\end{itemize}
\newpage
\begin{itemize}
    % TODO put hats on Gs
    \item Consider the set of expectation-component parameters $((r_1, G_1),
        (r_2, G_2), \dots (r_T, G_T))$, where all $r$ and $G$ are
        uncorrelated. Let $\epsilon = 0.2$.
    \item Let the probabilities $p(G_t > 0)$, $p(G_t < 0) = 0.5$, 
            $p(r_t > 1.2)$, $p(r_t < 0.8)$ be fixed.
    \item A particular timestep $t$ will be penalized in either of two
        cases:
    \begin{itemize}
        \item $G_t$ is positive and $r_t > 1.2$.
        \item $G_t$ is negative and $r_t < 0.8$.
    \end{itemize}
    \item Therefore, by the assumption of independence of $G_t$ and $r_t$, we
        have the expected number of penalized timesteps:\\
        $(p(G_t > 0)p(r_t > 1.2) + p(G_t < 0)p(r_t < 0.8))T$.
    \item Let $G_{1, t}$ and $G_{2, t}$ be two alterate estimators. To
        understand differences in the expected number of penalized timesteps as
        we modify the sign of $G$, define the ratio:
    \begin{align*}
        r_{diff} &= 
        \frac{(p(G_{1, t} > 0)p(r_t > 1.2) + p(G_{1, t} < 0)p(r_t < 0.8))T}
            {(p(G_{2, t} > 0)p(r_t > 1.2) + p(G_{2, t} < 0)p(r_t < 0.8))T}\\
        &=\frac{p(G_{1, t} > 0)p(r_t > 1.2) + p(G_{1, t} < 0)p(r_t < 0.8)}
            {p(G_{2, t} > 0)p(r_t > 1.2) + p(G_{2, t} < 0)p(r_t < 0.8)}
    \end{align*}
    \item If $p(r_t > 1.2) = p(r_t < 0.8)$, this ratio degenerates to
        $1$ regardless of the sign distributions of $G_1$ and $G_2$.
    \item Consider the example where 
        $p(G_{1, t} > 0) = p(G_{1, t} < 0) = 0.5$
        and
        $p(G_{2, t} > 0) = 1$, $p(G_{2, t} < 0) = 0$. Finding the conditions
        under which $r_{diff} > 1$:
        \begin{align*}
            r_{diff} &>  1\\
            \frac{0.5(p(r_t > 1.2) + p(r_t < 0.8))}
                {p(r_t > 1.2)}
            &> 1\\
            \frac{p(r_t > 1.2) + p(r_t < 0.8)}
                {p(r_t > 1.2)}
            &> 2\\
            p(r_t > 1.2) + p(r_t < 0.8)
            &> 2p(r_t > 1.2)\\
            p(r_t < 0.8)
            &> p(r_t > 1.2)
        \end{align*}
    \item Similarly, we have that if $p(G_{2, t} > 0) = 0$, 
        $p(G_{2, t} < 0) = 1$, we must have that $p(r_t < 0.8) < p(r_t > 1.2)$
    \item Is it possible for these probabilities to be lopsided in this way?
        Consider the two policies parameterized by $\pi_1$ and
        $\pi_2$, in a state space of the single state $s$. Can we have that 
        $p(\pi_1(s, a_t) < \pi_2(s, a_t)) \ne 0.5$?
    \item All this requires is that, in choosing a state at random based on the
        on-policy distribution (2), we expect to find that the sum of the
        2-probabilities of actions where the probability under 2 is greater
        than 1 is > 0.5.
    \item WTS: $p(r_t < 0) > p(r_t > 0)$, i.e., $p(r_t < 0) > 0.5$.
    \item $p(r_t < 0) = \sum_{s}p(s | \pi_{old})
        \sum_{a: \pi(a | s) < \pi_{old}(a | s)}\pi_{old}(a | s)$.
    \item Moving to a continuous action space, and considering gaussian
        policies with fixed standard deviations, we know that, at any
        particular state $s$, $\int_{a: \pi(a | s) < \pi_{old}(a |
        s)}\pi_{old}(a | s) \ge 0.5$.
    \item Therefore, in this situation, the above must be true.
    \end{itemize}

%\newpage
%\section*{Ideas and Intuitions}
%\subsection*{Sigmoid Min-Filters}
%
%\newpage
%\section*{Plan}

\newpage
\begin{thebibliography}{9}
\bibitem{ppo} 
\url{https://arxiv.org/abs/1707.06347}
\bibitem{gae} 
\url{https://arxiv.org/abs/1506.02438}
\bibitem{trpo} 
\url{https://arxiv.org/abs/1502.05477}
\end{thebibliography}

\end{document}
